{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to use cuda, you can specify the ID of the device\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/input/train.tsv', sep='\\t')\n",
    "df = df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3530, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic_comment</th>\n",
       "      <th>neutral_comment1</th>\n",
       "      <th>neutral_comment2</th>\n",
       "      <th>neutral_comment3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>и,чё,блядь где этот херой был до этого со свои...</td>\n",
       "      <td>Ну и где этот герой был,со своими доказательст...</td>\n",
       "      <td>Где этот герой был до этого со своими доказате...</td>\n",
       "      <td>и,где этот герой был до этого со своими доказа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>О, а есть деанон этого петуха?</td>\n",
       "      <td>О, а есть деанон</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>херну всякую пишут,из-за этого лайка.долбоебизм.</td>\n",
       "      <td>Чушь всякую пишут, из- за этого лайка.</td>\n",
       "      <td>Ерунду всякую пишут,из-за этого лайка.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>из за таких пидоров мы и страдаем</td>\n",
       "      <td>из за таких плохих людей мы и страдаем</td>\n",
       "      <td>Из-за таких людей мы и страдаем</td>\n",
       "      <td>из за таких как он мы и страдаем</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>гондон путинский он а не артист</td>\n",
       "      <td>Человек Путина он, а не артист</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       toxic_comment  \\\n",
       "0  и,чё,блядь где этот херой был до этого со свои...   \n",
       "1                     О, а есть деанон этого петуха?   \n",
       "2   херну всякую пишут,из-за этого лайка.долбоебизм.   \n",
       "3                  из за таких пидоров мы и страдаем   \n",
       "4                    гондон путинский он а не артист   \n",
       "\n",
       "                                    neutral_comment1  \\\n",
       "0  Ну и где этот герой был,со своими доказательст...   \n",
       "1                                   О, а есть деанон   \n",
       "2             Чушь всякую пишут, из- за этого лайка.   \n",
       "3             из за таких плохих людей мы и страдаем   \n",
       "4                     Человек Путина он, а не артист   \n",
       "\n",
       "                                    neutral_comment2  \\\n",
       "0  Где этот герой был до этого со своими доказате...   \n",
       "1                                                      \n",
       "2             Ерунду всякую пишут,из-за этого лайка.   \n",
       "3                    Из-за таких людей мы и страдаем   \n",
       "4                                                      \n",
       "\n",
       "                                    neutral_comment3  \n",
       "0  и,где этот герой был до этого со своими доказа...  \n",
       "1                                                     \n",
       "2                                                     \n",
       "3                   из за таких как он мы и страдаем  \n",
       "4                                                     "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ну и где этот герой был,со своими доказательствами?',\n",
       " 'Где этот герой был до этого со своими доказательствами?',\n",
       " 'и,где этот герой был до этого со своими доказательствами?']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0][['neutral_comment1', 'neutral_comment2', 'neutral_comment3']].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_toxic = []\n",
    "df_train_neutral = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    references = row[['neutral_comment1', 'neutral_comment2', 'neutral_comment3']].tolist()\n",
    "    \n",
    "    for reference in references:\n",
    "        if len(reference) > 0:\n",
    "            df_train_toxic.append(row['toxic_comment'])\n",
    "            df_train_neutral.append(reference)\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'toxic_comment': df_train_toxic,\n",
    "    'neutral_comment': df_train_neutral\n",
    "})\n",
    "\n",
    "df = shuffle(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing data structures for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert idx < len(self.x['input_ids'])\n",
    "        item = {key: val[idx] for key, val in self.x.items()}\n",
    "        item['decoder_attention_mask'] = self.y['attention_mask'][idx]\n",
    "        item['labels'] = self.y['input_ids'][idx]\n",
    "        return item\n",
    "    \n",
    "    @property\n",
    "    def n(self):\n",
    "        return len(self.x['input_ids'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n # * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.file_utils import cached_property\n",
    "from typing import Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Union\n",
    "\n",
    "class DataCollatorWithPadding:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        batch = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=True,\n",
    "        )\n",
    "        ybatch = self.tokenizer.pad(\n",
    "            {'input_ids': batch['labels'], 'attention_mask': batch['decoder_attention_mask']},\n",
    "            padding=True,\n",
    "        ) \n",
    "        batch['labels'] = ybatch['input_ids']\n",
    "        batch['decoder_attention_mask'] = ybatch['attention_mask']\n",
    "        \n",
    "        return {k: torch.tensor(v) for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_dataloader):\n",
    "    num = 0\n",
    "    den = 0\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        with torch.no_grad():\n",
    "            loss = model(**{k: v.to(model.device) for k, v in batch.items()}).loss\n",
    "            num += len(batch) * loss.item()\n",
    "            den += len(batch)\n",
    "    val_loss = num / den\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "    model, train_dataloader, val_dataloader, \n",
    "    max_epochs=30, \n",
    "    max_steps=1_000, \n",
    "    lr=3e-5,\n",
    "    gradient_accumulation_steps=1, \n",
    "    cleanup_step=100,\n",
    "    report_step=300,\n",
    "    window=100,\n",
    "):\n",
    "    cleanup()\n",
    "    optimizer = torch.optim.Adam(params = [p for p in model.parameters() if p.requires_grad], lr=lr)\n",
    "\n",
    "    ewm_loss = 0\n",
    "    step = 0\n",
    "    model.train()\n",
    "\n",
    "    for epoch in trange(max_epochs):\n",
    "        print(step, max_steps)\n",
    "        if step >= max_steps:\n",
    "            break\n",
    "        tq = tqdm(train_dataloader)\n",
    "        for i, batch in enumerate(tq):\n",
    "            try:\n",
    "                batch['labels'][batch['labels']==0] = -100\n",
    "                loss = model(**{k: v.to(model.device) for k, v in batch.items()}).loss\n",
    "                loss.backward()\n",
    "            except Exception as e:\n",
    "                print('error on step', i, e)\n",
    "                loss = None\n",
    "                cleanup()\n",
    "                continue\n",
    "            if i and i % gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                step += 1\n",
    "                if step >= max_steps:\n",
    "                    break\n",
    "\n",
    "            if i % cleanup_step == 0:\n",
    "                cleanup()\n",
    "\n",
    "            w = 1 / min(i+1, window)\n",
    "            ewm_loss = ewm_loss * (1-w) + loss.item() * w\n",
    "            tq.set_description(f'loss: {ewm_loss:4.4f}')\n",
    "\n",
    "            if (i and i % report_step == 0 or i == len(train_dataloader)-1)  and val_dataloader is not None:\n",
    "                model.eval()\n",
    "                eval_loss = evaluate_model(model, val_dataloader)\n",
    "                model.train()\n",
    "                print(f'epoch {epoch}, step {i}/{step}: train loss: {ewm_loss:4.4f}  val loss: {eval_loss:4.4f}')\n",
    "                \n",
    "            if step % 1000 == 0:\n",
    "                model.save_pretrained(f't5_base_{dname}_{steps}')\n",
    "        \n",
    "    cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x, y, model_name, test_size=0.1, batch_size=32, **kwargs):\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name).cuda()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    x1, x2, y1, y2 = train_test_split(x, y, test_size=test_size, random_state=42)\n",
    "    train_dataset = PairsDataset(tokenizer(x1), tokenizer(y1))\n",
    "    test_dataset = PairsDataset(tokenizer(x2), tokenizer(y2))\n",
    "    \n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, drop_last=False, shuffle=True, collate_fn=data_collator)\n",
    "    val_dataloader = DataLoader(test_dataset, batch_size=batch_size, drop_last=False, shuffle=True, collate_fn=data_collator)\n",
    "\n",
    "    train_loop(model, train_dataloader, val_dataloader, **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the type of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'sberbank-ai/ruT5-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    'train': df\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model! You can hyperparametrize the number of training iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for steps in [300, 1000, 3000, 10000]:\n",
    "    for dname, d in datasets.items():\n",
    "        print(f'\\n\\n\\n  {dname}  {steps} \\n=====================\\n\\n')\n",
    "        model = train_model(d['toxic_comment'].tolist(), d['neutral_comment'].tolist(), model_name=model_name, batch_size=16, max_epochs=1000, max_steps=steps)\n",
    "        model.save_pretrained(f't5_base_{dname}_{steps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
